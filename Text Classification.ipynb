{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZd0LJzbISPd"
   },
   "source": [
    "## Data Loading \n",
    "\n",
    "First, download the SMS Spam Collection Dataset. The dataset is taken from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset/data#) and loaded to [Google Drive](https://drive.google.com/open?id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "id": "PvGErs2oHkWU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-11 03:19:42--  https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR\n",
      "Resolving docs.google.com (docs.google.com)... 142.251.40.110\n",
      "Connecting to docs.google.com (docs.google.com)|142.251.40.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tf5sbh75u0t5rl7c19ec9qtab3dhg0r0/1644567525000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-02-11 03:19:42--  https://doc-14-04-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/tf5sbh75u0t5rl7c19ec9qtab3dhg0r0/1644567525000/08752484438609855375/*/1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR?e=download\n",
      "Resolving doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)... 142.251.40.97\n",
      "Connecting to doc-14-04-docs.googleusercontent.com (doc-14-04-docs.googleusercontent.com)|142.251.40.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 503663 (492K) [text/csv]\n",
      "Saving to: ‘spam.csv’\n",
      "\n",
      "spam.csv            100%[===================>] 491.86K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2022-02-11 03:19:43 (11.7 MB/s) - ‘spam.csv’ saved [503663/503663]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://docs.google.com/uc?export=download&id=1OVRo37agn02mc6yp5p6-wtJ8Hyb-YMXR' -O spam.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "id": "RcHV1lUwtH-n"
   },
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXVQCF-ovo4G"
   },
   "source": [
    "There are two columns: `v1` -- spam or ham indicator, `v2` -- text of the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "id": "BiKE89v0zMiY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", usecols=[\"v1\", \"v2\"], encoding='latin-1')\n",
    "# 1 - spam, 0 - ham\n",
    "df.v1 = (df.v1 == \"spam\").astype(\"int\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train, val, and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "id": "Ga5Qydpw-gdQ"
   },
   "outputs": [],
   "source": [
    "# 0.15 for val, 0.15 for test, 0.7 for train\n",
    "val_size = int(df.shape[0] * 0.15)\n",
    "test_size = int(df.shape[0] * 0.15)\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "\n",
    "indices = np.arange(int(df.shape[0]))\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "\n",
    "train_size = int(df.shape[0] * 0.7)\n",
    "train_val_size = train_size + val_size\n",
    "\n",
    "\n",
    "train_indices = shuffled_indices[0:train_size]\n",
    "dev_indices = shuffled_indices[train_size:train_val_size]\n",
    "test_indicies = shuffled_indices[train_val_size:]\n",
    "\n",
    "\n",
    "train = df.loc[train_indices]\n",
    "val = df.loc[dev_indices]\n",
    "test = df.loc[test_indicies]\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "train_texts, train_labels = train.loc[:,\"v2\"].tolist(), train.loc[:,\"v1\"]\n",
    "val_texts, val_labels     = val.loc[:,\"v2\"].tolist(), val.loc[:,\"v1\"]\n",
    "test_texts, test_labels  = test.loc[:,\"v2\"].tolist(), test.loc[:,\"v1\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGyHG4lBISP2"
   },
   "source": [
    "## Data Processing \n",
    "\n",
    "The task is to create bag-of-words features: tokenize the text, index each token, represent the sentence as a dictionary of tokens and their counts, limit the vocabulary to $n$ most frequent tokens. \n",
    "\n",
    "\n",
    "Function `preprocess_data` takes the list of texts and returns list of (lists of tokens). \n",
    "\n",
    "\n",
    "Class `Vectorizer` is used to vectorize the text and to create a matrix of features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/corrinazhang1220/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.punkt.PunktSentenceTokenizer at 0x7fe80f415f50>"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {
    "id": "793EFaQYhHeR"
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # This function returns a list of lists of preprocessed tokens for each message\n",
    "   \n",
    "    \n",
    "    preprocessed_data = []\n",
    "    for i in range(len(data)):\n",
    "        token_list = nltk.word_tokenize(data[i])\n",
    "        #change the words to lower cases and ignore the punctuations. \n",
    "        revised_list = [word.lower() for word in token_list if word.isalnum()]\n",
    "        preprocessed_data.append(token_list)\n",
    "        \n",
    "            \n",
    "    \n",
    "    \n",
    "    return preprocessed_data\n",
    "\n",
    "train_data = preprocess_data(train_texts)\n",
    "val_data = preprocess_data(val_texts)\n",
    "test_data = preprocess_data(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {
    "id": "TM2qpOKpjVbD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Vectorizer():\n",
    "    def __init__(self, max_features):\n",
    "        self.max_features = max_features\n",
    "        self.vocab_list = None\n",
    "        self.token_to_index = None\n",
    "        \n",
    "    def fit(self, dataset):\n",
    "        # Create a vocab list, self.vocab_list, using the most frequent \"max_features\" tokens\n",
    "        # Create a token indexer, self.token_to_index, that will map each token in self.vocab \n",
    "        # to its corresponding index in self.vocab_list\n",
    "      \n",
    "        \n",
    "        data = [x for line in dataset for x in line]\n",
    "        \n",
    "        most_frequent_words = Counter(data).most_common(self.max_features)\n",
    "        self.vocab_list = [token[0]for token in most_frequent_words]\n",
    "        self.token_to_index = {ch: i for i, ch in enumerate(self.vocab_list)}\n",
    "        \n",
    "    def transform(self, dataset):\n",
    "        # This function transforms text dataset into a matrix, data_matrix\n",
    "     \n",
    "        data_matrix = np.zeros((len(dataset), len(self.vocab_list)))\n",
    "        \n",
    "        for i in range(len(dataset)):\n",
    "            for word in dataset[i]:\n",
    "                if word in self.vocab_list:\n",
    "                    index = int(self.token_to_index[word])\n",
    "                    data_matrix[i][index] += 1 \n",
    "        \n",
    "        return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {
    "id": "wXMrZXlZjcH7"
   },
   "outputs": [],
   "source": [
    "max_features = 750 \n",
    "vectorizer = Vectorizer(max_features=max_features)\n",
    "vectorizer.fit(train_data)\n",
    "X_train = vectorizer.transform(train_data)\n",
    "X_val = vectorizer.transform(val_data)\n",
    "X_test = vectorizer.transform(test_data)\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "vocab = vectorizer.vocab_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wtm7a6JWu9-3"
   },
   "source": [
    "## Model\n",
    "\n",
    "We train logistic regression model and save prediction for train, val and test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {
    "id": "Wq9stSAbAIZe"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define Logistic Regression model\n",
    "model = LogisticRegression(random_state=0, solver='liblinear')\n",
    "\n",
    "# Fit the model to training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make prediction using the trained model\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_test_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Akg9LvP5DGE8"
   },
   "source": [
    "The task is to report train, val, test accuracies and F1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {
    "id": "chqVbKH6kZyY"
   },
   "outputs": [],
   "source": [
    "def accuracy_score(y_true, y_pred): \n",
    "    # Calculate accuracy of the model's prediction\n",
    "   \n",
    "    TP = np.sum(y_true & y_pred)\n",
    "    FP = np.sum(y_pred) - TP\n",
    "    TN = np.sum(y_true == 0 ) - FP\n",
    "    FN = np.sum(y_true) - TP\n",
    "    accuracy = (TP + TN)/(TP + FP + FN + TN)\n",
    "    return accuracy\n",
    "\n",
    "def f1_score(y_true, y_pred): \n",
    "    # Calculate F1 score of the model's prediction\n",
    "  \n",
    "    TP = np.sum(y_true & y_pred)\n",
    "    FP = np.sum(y_pred) - TP\n",
    "    TN = np.sum(y_true == 0 ) - FP\n",
    "    FN = np.sum(y_true) - TP\n",
    "    \n",
    "    f1 = TP/(TP + (FP + FN)/2)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {
    "id": "MqrMw0udDD04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.989, F1 score: 0.958\n",
      "Validation accuracy: 0.983, F1 score: 0.929\n",
      "Test accuracy: 0.982, F1 score: 0.934\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training accuracy: {accuracy_score(y_train, y_train_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"Validation accuracy: {accuracy_score(y_val, y_val_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_val, y_val_pred):.3f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred):.3f}, \"\n",
    "      f\"F1 score: {f1_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FW7P84giGgP4"
   },
   "source": [
    "**Question.**\n",
    "Is accuracy the metric that logistic regression optimizes while training? If no, which metric is optimized in logistic regression?\n",
    "\n",
    "**Your answer:** No. There are other metric that should be optimized while training. Precision = Of all positive predictions, how many are really positive. Recall = Of all real positive cases, how many are predicted positive. Precision measures the error caused by FP, and recall measures the error caused by FN. A model prefers both minimum FN and FP, as well as a balance between FN and FP. Thus, we want to optimize F1 score which measures the harmonic mean of FN and FP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak0h71krLPqX"
   },
   "source": [
    "**Question.**\n",
    "In general, does having 0.99 accuracy on test means that the model is great? If no, can you give an example of a case when the accuracy is high but the model is not good? (Hint: why do we use F1 score?)\n",
    "\n",
    "**Your answer:** \n",
    "No. If the dataset is imbalanced and 99%of the instances are postive, and 1% of the instances are negative. The model could just predict every instances to be positive, and get accuracy of 99%. However, this model is not good, since it does not predict base on features, instead it just predicts all the instances to be postive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_RDI0qdOxwM"
   },
   "source": [
    "### Exploration of predicitons "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHR2OqYCDOxs"
   },
   "source": [
    "Show a few examples with true+predicted labels on the train and val sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "id": "5yv8GD-UGXvR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 is truly predicted y_train: 0 y_train_pred: 0\n",
      "Wat makes some people dearer is not just de happiness dat u feel when u meet them but de pain u feel when u miss dem!!! \n",
      "\n",
      "index 1 is truly predicted y_train: 0 y_train_pred: 0\n",
      "Thanks for being there for me just to talk to on saturday. You are very dear to me. I cherish having you as a brother and role model. \n",
      "\n",
      "index 2 is truly predicted y_train: 0 y_train_pred: 0\n",
      "She was supposed to be but couldn't make it, she's still in town though \n",
      "\n",
      "index 4 is truly predicted y_train: 0 y_train_pred: 0\n",
      "Thank You for calling.Forgot to say Happy Onam to you Sirji.I am fine here and remembered you when i met an insurance person.Meet You in Qatar Insha Allah.Rakhesh, ex Tata AIG who joined TISSCO,Tayseer. \n",
      "\n",
      "index 5 is truly predicted y_train: 0 y_train_pred: 0\n",
      "K..then come wenever u lik to come and also tel vikky to come by getting free time..:-) \n",
      "\n",
      "index 6 is truly predicted y_train: 0 y_train_pred: 0\n",
      "I will cme i want to go to hos 2morow. After that i wil cme. This what i got from her dear what to do. She didnt say any time \n",
      "\n",
      "index 7 is truly predicted y_train: 0 y_train_pred: 0\n",
      "ALRITE \n",
      "\n",
      "index 8 is truly predicted y_train: 0 y_train_pred: 0\n",
      "Yeah jay's sort of a fucking retard \n",
      "\n",
      "index 9 is truly predicted y_train: 0 y_train_pred: 0\n",
      "Tell me something. Thats okay. \n",
      "\n",
      "index 10 is truly predicted y_train: 1 y_train_pred: 1\n",
      "Thanks for your ringtone order, reference number X29. Your mobile will be charged 4.50. Should your tone not arrive please call customer services 09065989180 \n",
      "\n",
      "index 0 is truly predicted y_val: 0 y_val_pred: 0\n",
      "I think your mentor is , but not 100 percent sure. \n",
      "\n",
      "index 1 is truly predicted y_val: 0 y_val_pred: 0\n",
      "I re-met alex nichols from middle school and it turns out he's dealing! \n",
      "\n",
      "index 2 is truly predicted y_val: 0 y_val_pred: 0\n",
      "Prepare to be pleasured :) \n",
      "\n",
      "index 3 is truly predicted y_val: 0 y_val_pred: 0\n",
      "Love isn't a decision, it's a feeling. If we could decide who to love, then, life would be much simpler, but then less magical \n",
      "\n",
      "index 4 is truly predicted y_val: 0 y_val_pred: 0\n",
      "I havent add Ì_ yet right..  \n",
      "\n",
      "index 5 is truly predicted y_val: 0 y_val_pred: 0\n",
      "Aight I'll grab something to eat too, text me when you're back at mu \n",
      "\n",
      "index 6 is truly predicted y_val: 0 y_val_pred: 0\n",
      "FRAN I DECIDED 2 GO N E WAY IM COMPLETELY BROKE AN KNACKERED I GOT UP BOUT 3 C U 2MRW LOVE JANX P.S THIS IS MY DADS FONE, -NO CREDIT \n",
      "\n",
      "index 7 is truly predicted y_val: 1 y_val_pred: 1\n",
      "U have a secret admirer who is looking 2 make contact with U-find out who they R*reveal who thinks UR so special-call on 09058094565 \n",
      "\n",
      "index 8 is truly predicted y_val: 0 y_val_pred: 0\n",
      "We live in the next  &lt;#&gt; mins \n",
      "\n",
      "index 9 is truly predicted y_val: 0 y_val_pred: 0\n",
      "I knew it... U slept v late yest? Wake up so late... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1 - spam, 0 - ham\n",
    "\n",
    "def true_pairs(y_true, y_pred):\n",
    "    true_predict = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            true_predict.append(i)\n",
    "    return true_predict\n",
    "        \n",
    "\n",
    "y_train_true_pairs = true_pairs(y_train, y_train_pred)\n",
    "y_val_true_pairs = true_pairs(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "for pair in (y_train_true_pairs[0:10]):\n",
    "    print(\"index\",pair, \"is truly predicted\", \"y_train:\", y_train[pair], \"y_train_pred:\", y_train_pred[pair])\n",
    "    print(train_texts[pair],\"\\n\")\n",
    "    \n",
    "    \n",
    "for pair in (y_val_true_pairs[0:10]):\n",
    "    print(\"index\",pair, \"is truly predicted\", \"y_val:\", y_val[pair], \"y_val_pred:\", y_val_pred[pair])\n",
    "    print(val_texts[pair],\"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neMQ4VR9GVL3"
   },
   "source": [
    "**Question** Print 10 examples from val set which were labeled incorrectly by the model. Why do you think the model got them wrong?\n",
    "\n",
    "**Your answer:** When I checked the raw contents from the val sets which were labeled incorrectly by the model. I found out that some of the contents contain some numbers or some random combinations of characters. These words did not appear in the bag of words because they rarely appear in all the texts. From this perspective, after vectorizer transform the text using \"bag of words\", these \"fail predicted\" text might not have shown something drastically differnt from other ham texts because the random combination of characters were not counted, and some other words that might appear in ham texts are counted. Thus, the model might not be able to lable these \"fail predicted\" text correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {
    "id": "7ssK0jRxGY3u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 163 fail predicted y_val: 1 y_val_pred: 0\n",
      "You can donate å£2.50 to UNICEF's Asian Tsunami disaster support fund by texting DONATE to 864233. å£2.50 will be added to your next bill \n",
      "\n",
      "index 171 fail predicted y_val: 0 y_val_pred: 1\n",
      "Derp. Which is worse, a dude who always wants to party or a dude who files a complaint about the three drug abusers he lives with \n",
      "\n",
      "index 196 fail predicted y_val: 0 y_val_pred: 1\n",
      "Hi! You just spoke to MANEESHA V. We'd like to know if you were satisfied with the experience. Reply Toll Free with Yes or No. \n",
      "\n",
      "index 198 fail predicted y_val: 1 y_val_pred: 0\n",
      "TBS/PERSOLVO. been chasing us since Sept forå£38 definitely not paying now thanks to your information. We will ignore them. Kath. Manchester. \n",
      "\n",
      "index 213 fail predicted y_val: 1 y_val_pred: 0\n",
      "SMS. ac sun0819 posts HELLO:\\You seem cool \n",
      "\n",
      "index 277 fail predicted y_val: 1 y_val_pred: 0\n",
      "England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/Ì¼1.20 POBOXox36504W45WQ 16+ \n",
      "\n",
      "index 299 fail predicted y_val: 1 y_val_pred: 0\n",
      "Buy Space Invaders 4 a chance 2 win orig Arcade Game console. Press 0 for Games Arcade (std WAP charge) See o2.co.uk/games 4 Terms + settings. No purchase \n",
      "\n",
      "index 506 fail predicted y_val: 1 y_val_pred: 0\n",
      "You will recieve your tone within the next 24hrs. For Terms and conditions please see Channel U Teletext Pg 750 \n",
      "\n",
      "index 586 fail predicted y_val: 1 y_val_pred: 0\n",
      "Check Out Choose Your Babe Videos @ sms.shsex.netUN fgkslpoPW fgkslpo \n",
      "\n",
      "index 608 fail predicted y_val: 1 y_val_pred: 0\n",
      "(Bank of Granite issues Strong-Buy) EXPLOSIVE PICK FOR OUR MEMBERS *****UP OVER 300% *********** Nasdaq Symbol CDGT That is a $5.00 per.. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fail_pairs(y_true, y_pred):\n",
    "    fail_predict = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != y_pred[i]:\n",
    "            fail_predict.append(i)\n",
    "    return fail_predict\n",
    "        \n",
    "\n",
    "y_val_fail_pairs = fail_pairs(y_val, y_val_pred)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "for pair in (y_val_fail_pairs[0:10]):\n",
    "    print(\"index\",pair, \"fail predicted\", \"y_val:\", y_val[pair], \"y_val_pred:\", y_val_pred[pair])\n",
    "    print(val_texts[pair],\"\\n\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSGA1012-HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
